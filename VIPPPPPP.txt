import os
import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from sentence_transformers import SentenceTransformer 

# --- CONFIGURATION ---
# ⚠️ MAKE SURE YOUR KEY IS PASTED HERE
GOOGLE_API_KEY = "AIzaSyAT3AOHDbw-T4L6pJoZgDl_wzAW080EgRU"

# Set env var for the library
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
DB_PATH = "./chroma_db"

def load_rag_resources():
    print("Loading Text Models...")
    text_embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    print("Loading Vector DB...")
    text_db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=text_embedding,
        collection_name="mkdocs_collection"
    )
    
    print("Loading Gemini...")
    # UPDATED: Using 'gemini-1.5-flash-latest' to avoid 404 errors
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.3
    )
    
    print("Loading Image Model (CLIP)...")
    clip_model = SentenceTransformer('clip-ViT-B-32')
    
    chroma_client = chromadb.PersistentClient(path=DB_PATH)
    try:
        image_collection = chroma_client.get_collection("mkdocs_images")
    except:
        image_collection = None
        print("⚠️ Warning: Image collection not found. (Did you run 2_ingest_images.py?)")
    
    return {
        "text_db": text_db,
        "llm": llm,
        "clip_model": clip_model,
        "image_collection": image_collection
    }

def ask_gemini(question, resources):
    text_db = resources["text_db"]
    llm = resources["llm"]
    clip_model = resources["clip_model"]
    image_collection = resources["image_collection"]
    
    # 1. Text Search
    retriever = text_db.as_retriever(search_kwargs={"k": 15})
    docs = retriever.invoke(question)
    context_text = "\n\n".join([doc.page_content for doc in docs])
    
    # 2. Image Search
    best_image = None
    if image_collection:
        try:
            # Encode question to vector
            query_embedding = clip_model.encode(question).tolist()
            # Search DB
            image_results = image_collection.query(
                query_embeddings=[query_embedding],
                n_results=1 
            )
            # Threshold check (lower distance = better match)
            if image_results['ids'] and image_results['distances'][0][0] < 3.0: 
                best_image = image_results['metadatas'][0][0]['path']
        except Exception as e:
            print(f"Image search skipped: {e}")
    
    # 3. Generate Answer
    template = """
    You are a friendly and expert technical assistant for MkDocs. 
    Your goal is to provide comprehensive, accurate answers based on the context.

    Guidelines:
    1. Use the provided Context to answer the Question.
    2. If the answer requires code, strictly use the code examples from the context.
    3. If the context contains multiple related pieces of info (e.g., flags, commands), synthesize them into a list.
    4. If the exact answer isn't in the context, state what IS known relative to the topic.
    
    Context:
    {context}
    
    Question: 
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm
    response = chain.invoke({"context": context_text, "question": question})
    
    return response.content, docs, best_image