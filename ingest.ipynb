{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8960b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to the specific docs folder inside the cloned repo\n",
    "DOCS_PATH = \"./mkdocs/docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc525f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning function defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_mkdocs_content(text):\n",
    "    \"\"\"\n",
    "    Cleans raw markdown text from MkDocs specific artifacts.\n",
    "    \"\"\"\n",
    "    # 1. Remove YAML Frontmatter (metadata between --- and --- at start)\n",
    "    # This regex looks for --- at start, followed by content, ending with ---\n",
    "    text = re.sub(r'^---\\n(.*?)\\n---\\n', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # 2. Remove \"admonition\" syntax but keep content (e.g., !!! note \"Title\")\n",
    "    # This removes the !!! type \"Title\" line\n",
    "    text = re.sub(r'!!! [a-z]+ \"(.*)\"', r'\\1:', text)\n",
    "    text = re.sub(r'!!! [a-z]+', '', text)\n",
    "    \n",
    "    # 3. Standardize whitespace (remove excessive newlines)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Cleaning function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7718cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 19\n",
      "Total chunks created: 736\n",
      "--- Example Chunk ---\n",
      "whenever anything in the configuration file, documentation directory, or theme\n",
      "directory changes.  \n",
      "Open the `docs/index.md` document in your text editor of choice, change the\n",
      "initial heading to `MkLorum`, and save your changes. Your browser will\n",
      "auto-reload and you should see your updated documentation immediately.  \n",
      "Now try editing the configuration file: `mkdocs.yml`. Change the\n",
      "[`site_name`][site_name] setting to `MkLorum` and save the file.  \n",
      "```yaml\n",
      "site_name: MkLorum\n",
      "```\n",
      "{'Header 1': 'Getting Started with MkDocs', 'Header 2': 'Creating a new project', 'source': 'mkdocs\\\\docs\\\\getting-started.md'}\n"
     ]
    }
   ],
   "source": [
    "# 1. Load all .md files (guard against missing directory)\n",
    "if os.path.exists(DOCS_PATH):\n",
    "    loader = DirectoryLoader(DOCS_PATH, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})\n",
    "    raw_documents = loader.load()\n",
    "else:\n",
    "    print(f\"Directory not found: {DOCS_PATH!r}. Skipping loading documents.\")\n",
    "    raw_documents = []\n",
    "\n",
    "# 2. Apply Cleaning (only if documents were loaded)\n",
    "if raw_documents:\n",
    "    for doc in raw_documents:\n",
    "        doc.page_content = clean_mkdocs_content(doc.page_content)\n",
    "\n",
    "    # 3. Define Headers to Split On (The Strategy)\n",
    "    # We want to keep sections together.\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    # 4. Process documents\n",
    "    md_header_splits = []\n",
    "    for doc in raw_documents:\n",
    "        # Split the document based on headers\n",
    "        splits = markdown_splitter.split_text(doc.page_content)\n",
    "\n",
    "        # Add the file path metadata back to these chunks (important for citations!)\n",
    "        for split in splits:\n",
    "            split.metadata[\"source\"] = doc.metadata.get(\"source\", None)\n",
    "\n",
    "        md_header_splits.extend(splits)\n",
    "\n",
    "    # 5. Secondary Split (Refinement)\n",
    "    # Sometimes a header section is still too long for the embedding model.\n",
    "    # We do a secondary split purely on character count to ensure they fit.\n",
    "    chunk_size = 500\n",
    "    chunk_overlap = 50\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    # Final Chunks\n",
    "    final_chunks = text_splitter.split_documents(md_header_splits)\n",
    "else:\n",
    "    md_header_splits = []\n",
    "    final_chunks = []\n",
    "\n",
    "print(f\"Total documents loaded: {len(raw_documents)}\")\n",
    "print(f\"Total chunks created: {len(final_chunks)}\")\n",
    "print(\"--- Example Chunk ---\")\n",
    "if len(final_chunks) > 5:\n",
    "    print(final_chunks[5].page_content)\n",
    "    print(final_chunks[5].metadata)\n",
    "else:\n",
    "    print(\"Not enough chunks available to show an example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361cc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vector DB... this might take a minute...\n",
      "✅ Vector DB Created and Saved to ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 1. Initialize Embedding Model (Deliverable 3)\n",
    "# \"all-MiniLM-L6-v2\" is fast, free, and runs locally.\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Create Vector DB (Deliverable 4)\n",
    "# This processes the 736 chunks and saves them to the './chroma_db' folder.\n",
    "print(\"Creating Vector DB... this might take a minute...\")\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=final_chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    collection_name=\"mkdocs_collection\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vector DB Created and Saved to ./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564d224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Query Results ---\n",
      "\n",
      "Result 1:\n",
      "Content: Altering a theme to suit your needs.  \n",
      "---  \n",
      "If you would like to make a few tweaks to an existing theme, there is no need\n",
      "to create your own theme from scratch. For minor tweaks which only require\n",
      "so...\n",
      "Source: mkdocs\\docs\\user-guide\\customizing-your-theme.md\n",
      "\n",
      "Result 2:\n",
      "Content: > is required for the theme.  \n",
      "[Customizing Your Theme]: ../user-guide/customizing-your-theme.md#using-the-theme-custom_dir\n",
      "[custom_dir]: ../user-guide/configuration.md#custom_dir\n",
      "[name]: ../user-guid...\n",
      "Source: mkdocs\\docs\\dev-guide\\themes.md\n",
      "\n",
      "Result 3:\n",
      "Content: <p class=\"card-text\">\n",
      "There's a stack of good looking <a href=\"user-guide/choosing-your-theme\">themes</a> available for MkDocs.\n",
      "Choose between the built in themes:\n",
      "<a href=\"user-guide/choosing-your-th...\n",
      "Source: mkdocs\\docs\\index.md\n"
     ]
    }
   ],
   "source": [
    "# Test Query to verify everything works\n",
    "query = \"How do I change the theme?\"\n",
    "results = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "print(\"\\n--- Test Query Results ---\")\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"Content: {res.page_content[:200]}...\") # Show first 200 chars\n",
    "    print(f\"Source: {res.metadata.get('source')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c3ab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid GOOGLE_API_KEY found in environment.\n",
      "Searching for available models...\n",
      "Failed to list models. Please verify your API key and network connectivity.\n",
      "Error details: Model.__init__() got an unexpected keyword argument 'thinking'\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. Setup - prefer environment variable, but avoid using the placeholder value\n",
    "env_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not env_key or env_key == \"AIzaSyAT3AOHDbw-T4L6pJoZgDl_wzAW080EgRU\":\n",
    "    print(\"No valid GOOGLE_API_KEY found in environment.\")\n",
    "    # Prompt the user to securely enter the API key (hidden input)\n",
    "    key = getpass.getpass(\"Enter your Google Generative AI API key (input hidden): \").strip()\n",
    "    if not key:\n",
    "        raise RuntimeError(\"No API key provided. Set the GOOGLE_API_KEY environment variable or enter a key when prompted.\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# 2. List all available models\n",
    "print(\"Searching for available models...\")\n",
    "try:\n",
    "    for m in genai.list_models():\n",
    "        if 'generateContent' in getattr(m, \"supported_generation_methods\", []):\n",
    "            print(f\"✅ Found: {m.name}\")\n",
    "except Exception as e:\n",
    "    # Provide a clearer error message to help debugging invalid API keys\n",
    "    print(\"Failed to list models. Please verify your API key and network connectivity.\")\n",
    "    print(\"Error details:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
